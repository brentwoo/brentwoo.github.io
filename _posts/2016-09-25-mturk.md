---
layout: post
title: "First foray into Mechanical Turk for Experimental Syntax"
date: 2016-09-25
---


<h2>Background prep work</h2>

In a mad rush of productivity, I spent my Saturday learning how to use Mechanical Turk to run a syntax experiment. I had already read enough of the background literature, including <a href="http://www.colinphillips.net/wp-content/uploads/2014/08/phillips2010_armchairlinguistics.pdf">Colin Phillips' (2010) 'Should we impeach armchair linguists'</a>, Wayne Cowart's <i>Experimental Syntax</i>, <a href="http://languagelog.ldc.upenn.edu/nll/?p=1233">Chris Potts' post on Language Log 'In defense of Amazon's Mechanical Turk', and a dozen other articles from Sprouse, Gibson, Fedorenko, etc. to feel justified doing experimental work like this -- collecting grammaticality judgments from non-linguist, untrained populations.

<div style="
    width: 100%;
    padding: 10px;
    border: 1px solid gray;
    margin: 5px;"><b>Quick, what is Mechanical Turk?</b> Mega-crowdsourcing of surveys. You can post your survey (or experiment stimuli) online to thousands of active users, and pay them. Everything is done online, with a fairly intuitive clicky-through interface. Obviously nice for commerical uses, but some clever people found that it's great for linguistics research!<br><br><b>HIT:</b> Human Intelligence Tasks. A single HIT is like asking "Is this one sentence grammatical, Y/N?". From what I gathered, the going rate for a single HIT is $0.005-$0.02.<br><b>Worker:</b> people who take your survey (or answer your question).<b>Requester</b>: the "researcher". probably you. </div>

I followed <a href="http://mturk.mit.edu/tutorial/">Morris Alper's step-by-step MTurk Tutorial</a> to learn how to use the sandbox and practice making experiments. This took a lot of trial and error. I don't think Amazon's setup is that great - the sandbox is just like the real thing. Which unfortunately means there's some delay between publishing your experiment and viewing it, and you have to create separate accounts to administer and test your experiments. But it works. Highly recommended to have a dual monitor (at least) set up.

I was relieved to find that designing the presentation of an experiment is done in HTML! Familiar radio button, form, formatting syntax. Whew. It didn't seem to like Javascript for making a multi-page quiz (the above MTurk Tutorial's Generator produces a multi-page quiz), so I had to present all my stimuli on one single page.

If you want real people responses, you do have to pay them. MTurk requires that you "pre-pay", which means putting some money in your account. 

I made good use of <a href="https://www.reddit.com/r/mturk/">Reddit's Mechanical Turk subreddit</a> and searched for tips for Requesters on payment. What I gathered from that forum is a glimpse into the mentality of the population. The typical MTurk Worker is doing things at a <i>very fast</i> pace (sometimes using programs to queue up and find the most efficient surveys). They will pass up surveys that appear to take too much time or pay too little (even though we're talking about cents, here). Workers are operating very quickly so you have to design accordingly. I don't think they'd be sympathetic to long introductory texts, or tasks that require 'training'. You just have to give them the data and ask them to react. The other thing I saw is that tasks paying $1 or $2 are seen as gold. If your task requires a longer survey, say you want 20 judgments from a single individual, worry not - as long as you can compensate accordingly, you will get responses. Lest you think this mentality degrades the quality of the responses, see the Language Log post above for discussion of that. It has been proven in meta-studies again and again that Workers respond just the same as any other population, like a traditional sit-down paper survey that you administer in person.

<h2>Making my own survey</h2>

Once I went through the survey and I made an ugly survey asking people for 4 grammatical judgments. I posted it, paying a Worker $0.02 if he answers completely. I was ambitious and set my "Desired Number of Responses" (in MTurk terms, "Assignments") at 50. For the first 30 minutes, I got 2 responses. This was exciting, but not nearly as fast as others were saying it should be, and the software projected that it would be completed sometime next morning. Hm. 

So I canceled that project, and made a similar one. I cut down on some of the introductory fluff text, upped the pay to $0.05, and made it live. 

In the first hour I had 13 responses! I was bouncing off the walls at this point - very exciting. Even more exciting -- I clicked through to see my results, and they were generally supporting my intuition. There is conducting an experiment and seeing it confirm your hunch. I went to dinner, and when I got back, it had finished! <b><u>I had received 50 responses to my survey, each providing 4 grammatical judgments, in about 3.5 hours. And it cost me $3.50.</u></b> And this is after maybe 5 hours of reading, designing, testing the experiment in the sandbox, and making it live!

What were my results? Well, I think it's safe to share the boxplot, made in RStudio:

<img src="{{ site.url }}/assets/slash-boxplot.png">

But to hear about the results, you'll have to wait for my <a href="http://www.lsuga.com/conference/2016">LCUGA 3 Proceedings article</a> to come out!
